{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51a83742",
   "metadata": {
    "papermill": {
     "duration": 0.013269,
     "end_time": "2023-01-28T23:37:01.738532",
     "exception": false,
     "start_time": "2023-01-28T23:37:01.725263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# *Part 1 - Text data and known labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6c30bcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:01.780278Z",
     "iopub.status.busy": "2023-01-28T23:37:01.779269Z",
     "iopub.status.idle": "2023-01-28T23:37:02.908394Z",
     "shell.execute_reply": "2023-01-28T23:37:02.907026Z"
    },
    "papermill": {
     "duration": 1.143857,
     "end_time": "2023-01-28T23:37:02.911656",
     "exception": false,
     "start_time": "2023-01-28T23:37:01.767799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read dataset \n",
    "import pandas as pd\n",
    "# visualize dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65974b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:02.951973Z",
     "iopub.status.busy": "2023-01-28T23:37:02.951591Z",
     "iopub.status.idle": "2023-01-28T23:37:03.020719Z",
     "shell.execute_reply": "2023-01-28T23:37:03.019557Z"
    },
    "papermill": {
     "duration": 0.082987,
     "end_time": "2023-01-28T23:37:03.023778",
     "exception": false,
     "start_time": "2023-01-28T23:37:02.940791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## training dataset\n",
    "dataset_train = pd.read_csv('EA-train.txt', header=None, names= ['text', 'emotion'] ,delimiter=';', quoting=3)\n",
    "# quoting: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35205dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first five rows\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  emotion\n",
       "15995  i just had a very brief time in the beanbag an...  sadness\n",
       "15996  i am now turning and i feel pathetic that i am...  sadness\n",
       "15997                     i feel strong and good overall      joy\n",
       "15998  i feel like this was such a rude comment and i...    anger\n",
       "15999  i know a lot but i feel so stupid because i ca...  sadness"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last five rows\n",
    "dataset_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58917e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (rows, cols)\n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582371c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe data\n",
    "dataset_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c64a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data\n",
    "dataset_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sns.countplot(x=dataset_train['emotion'],data=dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ba78138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:03.107546Z",
     "iopub.status.busy": "2023-01-28T23:37:03.107041Z",
     "iopub.status.idle": "2023-01-28T23:37:03.125289Z",
     "shell.execute_reply": "2023-01-28T23:37:03.124019Z"
    },
    "papermill": {
     "duration": 0.0323,
     "end_time": "2023-01-28T23:37:03.127872",
     "exception": false,
     "start_time": "2023-01-28T23:37:03.095572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test dataset\n",
    "dataset_test = pd.read_csv('EA-test.txt', header=None, names= ['text', 'emotion'] ,delimiter=';', quoting=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4851be58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:03.148781Z",
     "iopub.status.busy": "2023-01-28T23:37:03.148161Z",
     "iopub.status.idle": "2023-01-28T23:37:03.155689Z",
     "shell.execute_reply": "2023-01-28T23:37:03.154300Z"
    },
    "papermill": {
     "duration": 0.020632,
     "end_time": "2023-01-28T23:37:03.158153",
     "exception": false,
     "start_time": "2023-01-28T23:37:03.137521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (rows, cols)\n",
    "dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1aca2e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:03.208418Z",
     "iopub.status.busy": "2023-01-28T23:37:03.207998Z",
     "iopub.status.idle": "2023-01-28T23:37:03.227856Z",
     "shell.execute_reply": "2023-01-28T23:37:03.226389Z"
    },
    "papermill": {
     "duration": 0.033433,
     "end_time": "2023-01-28T23:37:03.230255",
     "exception": false,
     "start_time": "2023-01-28T23:37:03.196822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0  im feeling rather rotten so im not very ambiti...  sadness\n",
       "1          im updating my blog because i feel shitty  sadness\n",
       "2  i never make her separate from me because i do...  sadness\n",
       "3  i left with my bouquet of red and yellow tulip...      joy\n",
       "4    i was feeling a little vain when i did this one  sadness"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first five rows\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4c4823f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>i just keep feeling like someone is being unki...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>im feeling a little cranky negative after this...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>i feel that i am useful to my people and that ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>im feeling more comfortable with derby i feel ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>i feel all weird when i have to meet w people ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text emotion\n",
       "1995  i just keep feeling like someone is being unki...   anger\n",
       "1996  im feeling a little cranky negative after this...   anger\n",
       "1997  i feel that i am useful to my people and that ...     joy\n",
       "1998  im feeling more comfortable with derby i feel ...     joy\n",
       "1999  i feel all weird when i have to meet w people ...    fear"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last five rows\n",
    "dataset_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5fcd8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0                            i didnt feel humiliated        0\n",
       "1  i can go from feeling so hopeless to so damned...        0\n",
       "2   im grabbing a minute to post i feel greedy wrong        1\n",
       "3  i am ever feeling nostalgic about the fireplac...        2\n",
       "4                               i am feeling grouchy        1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace labels with integer numbers\n",
    "dataset_train['emotion'] = dataset_train['emotion'].replace({'sadness':0, 'anger':1, 'love':2, 'surprise':3, 'fear':4, 'joy':5}).astype(int)\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa0200fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0  im feeling rather rotten so im not very ambiti...        0\n",
       "1          im updating my blog because i feel shitty        0\n",
       "2  i never make her separate from me because i do...        0\n",
       "3  i left with my bouquet of red and yellow tulip...        5\n",
       "4    i was feeling a little vain when i did this one        0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace labels with integer numbers\n",
    "dataset_test['emotion'] = dataset_test['emotion'].replace({'sadness':0, 'anger':1, 'love':2, 'surprise':3, 'fear':4, 'joy':5}).astype(int)\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42045e67",
   "metadata": {
    "papermill": {
     "duration": 0.009298,
     "end_time": "2023-01-28T23:37:03.298922",
     "exception": false,
     "start_time": "2023-01-28T23:37:03.289624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "259bc673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       0\n",
       "emotion    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afac0870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:03.354574Z",
     "iopub.status.busy": "2023-01-28T23:37:03.353829Z",
     "iopub.status.idle": "2023-01-28T23:37:03.363032Z",
     "shell.execute_reply": "2023-01-28T23:37:03.361662Z"
    },
    "papermill": {
     "duration": 0.02377,
     "end_time": "2023-01-28T23:37:03.365883",
     "exception": false,
     "start_time": "2023-01-28T23:37:03.342113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       0\n",
       "emotion    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6dc0306",
   "metadata": {
    "papermill": {
     "duration": 0.009996,
     "end_time": "2023-01-28T23:37:03.386281",
     "exception": false,
     "start_time": "2023-01-28T23:37:03.376285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# *Part 2 - Data preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d714a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for NLP \n",
    "import re   # regular expressions: to select specific pattern\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f07cf2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im', 'updating', 'my', 'blog', 'because', 'i', 'feel', 'shitty']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "text_in= \"IM updating my blog because i feel shitty\"\n",
    "text_out = re.sub('[^a-zA-z]', ' ', text_in)\n",
    "print(word_tokenize(text_out.lower()))\n",
    "print(type(text_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b3cfc16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:03.409700Z",
     "iopub.status.busy": "2023-01-28T23:37:03.408542Z",
     "iopub.status.idle": "2023-01-28T23:37:14.348161Z",
     "shell.execute_reply": "2023-01-28T23:37:14.346952Z"
    },
    "papermill": {
     "duration": 10.954406,
     "end_time": "2023-01-28T23:37:14.351057",
     "exception": false,
     "start_time": "2023-01-28T23:37:03.396651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    corpus = []  # list to include the cleaned text in.\n",
    "\n",
    "    for row in range(data.shape[0]): # number of rows \n",
    "        # replace everything in the text that is not a letter by space, in every row in column text \n",
    "        text = re.sub('[^a-zA-z]', ' ', data['text'][row])\n",
    "\n",
    "        # convert text to tokens\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "\n",
    "        # remove stop words\n",
    "        stop_words = stopwords.words(\"english\")\n",
    "        stop_words.remove('not')  ## remove (not) from stopwords\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Word Normalization\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # convert tokens to text\n",
    "        text = ' '.join(tokens)\n",
    "        corpus.append(text)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c4f04a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the training text\n",
    "corpus_train = preprocess_text(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28d09fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7093a79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'didnt feel humili'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train[0]\n",
    "# didnt ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "69aed349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:14.405374Z",
     "iopub.status.busy": "2023-01-28T23:37:14.404569Z",
     "iopub.status.idle": "2023-01-28T23:37:15.613310Z",
     "shell.execute_reply": "2023-01-28T23:37:15.612242Z"
    },
    "papermill": {
     "duration": 1.223144,
     "end_time": "2023-01-28T23:37:15.616048",
     "exception": false,
     "start_time": "2023-01-28T23:37:14.392904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## cleaning the test text\n",
    "corpus_test = preprocess_text(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "636a27e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:15.638955Z",
     "iopub.status.busy": "2023-01-28T23:37:15.638523Z",
     "iopub.status.idle": "2023-01-28T23:37:15.644673Z",
     "shell.execute_reply": "2023-01-28T23:37:15.643706Z"
    },
    "papermill": {
     "duration": 0.020349,
     "end_time": "2023-01-28T23:37:15.646739",
     "exception": false,
     "start_time": "2023-01-28T23:37:15.626390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'im feel rather rotten im not ambiti right'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test[0]\n",
    "# im ??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8abce0e6",
   "metadata": {},
   "source": [
    "# *Part 3 - Feature Extraction*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cdaccc3",
   "metadata": {
    "papermill": {
     "duration": 0.009928,
     "end_time": "2023-01-28T23:37:15.667087",
     "exception": false,
     "start_time": "2023-01-28T23:37:15.657159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "528821c8",
   "metadata": {},
   "source": [
    "+ we can perform feature extraction using the CountVectorizer class from the scikit-learn library.\n",
    "+ represents text as a bag of individual words, without considering the order or structure of the text.\n",
    "+ CountVectorizer counts the frequency of each word in a text corpus and represents each document in the corpus as a vector of word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55f0cef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:37:15.689729Z",
     "iopub.status.busy": "2023-01-28T23:37:15.689016Z",
     "iopub.status.idle": "2023-01-28T23:37:16.316203Z",
     "shell.execute_reply": "2023-01-28T23:37:16.314937Z"
    },
    "papermill": {
     "duration": 0.641824,
     "end_time": "2023-01-28T23:37:16.319208",
     "exception": false,
     "start_time": "2023-01-28T23:37:15.677384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit train set\n",
    "X_train = vectorizer.fit_transform(corpus_train)\n",
    "y_train = dataset_train['emotion']\n",
    "\n",
    "# Transform test set\n",
    "X_test = vectorizer.transform(corpus_test)\n",
    "y_test = dataset_test['emotion']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b3577f1",
   "metadata": {
    "papermill": {
     "duration": 0.01003,
     "end_time": "2023-01-28T23:37:16.405924",
     "exception": false,
     "start_time": "2023-01-28T23:37:16.395894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# *Part 4 - The machine learning models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e5c6fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "# to find the best hyperparameters for the Naive Bayes model\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8a8d230",
   "metadata": {
    "papermill": {
     "duration": 0.010383,
     "end_time": "2023-01-28T23:37:16.477103",
     "exception": false,
     "start_time": "2023-01-28T23:37:16.466720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85b5ce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/elsayedelmandoh/.local/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 1198, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 1110, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/elsayedelmandoh/.local/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 1198, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 1110, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 32\u001b[0m\n\u001b[1;32m     25\u001b[0m random_search \u001b[39m=\u001b[39m RandomizedSearchCV(pipeline, \n\u001b[1;32m     26\u001b[0m                                    param_distributions\u001b[39m=\u001b[39mparams, \n\u001b[1;32m     27\u001b[0m                                    n_iter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[1;32m     28\u001b[0m                                    cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\u001b[39m# number of folds to use for cross-validation \u001b[39;00m\n\u001b[1;32m     29\u001b[0m                                    n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m# number of jobs to run in parallel: -1 is run GPU, 1 is run CPU\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# fit randomized search on training data and labels.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m random_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/utils/validation.py:72\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mPass \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m as keyword args. From version 0.25 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mpassing these as positional arguments will \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mresult in an error\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     70\u001b[0m                   \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m kwargs\u001b[39m.\u001b[39mupdate({k: arg \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/model_selection/_search.py:765\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    763\u001b[0m refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    766\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py:330\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[39mFit all the transforms one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39m    This estimator\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    329\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 330\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    331\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m'\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    332\u001b[0m                          \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    333\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py:292\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    290\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    291\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    293\u001b[0m     cloned_transformer, X, y, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    294\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    295\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    296\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name])\n\u001b[1;32m    297\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py:740\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    739\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m'\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 740\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    741\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    742\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py:1198\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1195\u001b[0m min_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_df\n\u001b[1;32m   1196\u001b[0m max_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_features\n\u001b[0;32m-> 1198\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents,\n\u001b[1;32m   1199\u001b[0m                                   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1202\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py:1110\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1109\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1111\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1112\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py:104\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/scipy/sparse/_base.py:761\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[1;32m    760\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "# define pipeline with the CountVectorizer and Naive Bayes classifier\n",
    "''' \n",
    "# Build the pipeline\n",
    "makes it easier to train and evaluate the model\n",
    "which allows you to reuse same preprocessing \n",
    "                    reuse same modeling steps on new data  \n",
    "            without retrain the preprocessing steps\n",
    "'''\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# define parameters grid with different values for hyperparameters\n",
    "params = {\n",
    "    'bow__ngram_range': [(1, 1), (1, 2), (2, 2)],# determines the range of n-grams to be used for tokenization.\n",
    "    # increasing the value of \"max_df\" to exclude terms that appear too frequently in the corpus.\n",
    "    'bow__max_df': [0.5, 0.75, 1.0],# minimum document frequency of a term in the corpus\n",
    "    # decreasing the value of \"min_df\" to allow more terms to be included,\n",
    "    'bow__min_df': [1, 2, 3],# maximum document frequency of a term in the corpus\n",
    "    'nb__alpha': [0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "# define randomized search\n",
    "random_search = RandomizedSearchCV(pipeline, \n",
    "                                   param_distributions=params, \n",
    "                                   n_iter=10, \n",
    "                                   cv=5,# number of folds to use for cross-validation \n",
    "                                   n_jobs=-1)# number of jobs to run in parallel: -1 is run GPU, 1 is run CPU\n",
    "\n",
    "# fit randomized search on training data and labels.\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for number of iteration\n",
    "pd.DataFrame(random_search.cv_results_)[[\"mean_test_score\",\"std_test_score\",\"params\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781176eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "+ The best hyperparameters and score found by the random search object can be accessed using: \n",
    "    - the best_params_ attribute\n",
    "    - The best_score_ attribute \n",
    "'''\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we can train the Naive Bayes model on the training data and labels \n",
    "using the pipeline with the best hyperparameters.\n",
    "'''\n",
    "best_pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(ngram_range=random_search.best_params_['bow__ngram_range'], \n",
    "                            max_df=random_search.best_params_['bow__max_df'], \n",
    "                            min_df=random_search.best_params_['bow__min_df'])),\n",
    "    ('nb', MultinomialNB(alpha=random_search.best_params_['nb__alpha']))\n",
    "])\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e5cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:43:46.036542Z",
     "iopub.status.busy": "2023-01-28T23:43:46.036103Z",
     "iopub.status.idle": "2023-01-28T23:43:46.203227Z",
     "shell.execute_reply": "2023-01-28T23:43:46.201964Z"
    },
    "papermill": {
     "duration": 0.192467,
     "end_time": "2023-01-28T23:43:46.207417",
     "exception": false,
     "start_time": "2023-01-28T23:43:46.014950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make predictions on test data using the trained model\n",
    "y_pred = best_pipeline.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5349fbd8",
   "metadata": {},
   "source": [
    "# part 5 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7aab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy score with y-test and y-predictions\n",
    "# number of correct predictions divided by the total number of predictions.\n",
    "accuracy = accuracy_score(dataset_test['emotion'], y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8f437ef",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "+ confusion matrix: Table that shows the \n",
    "    - true positive     - true negative\n",
    "    - false positive    - false negative\n",
    "    - values to predict actual values and create heatmap\n",
    "'''\n",
    "# compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd0de9ba",
   "metadata": {},
   "source": [
    "# part 6 - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ba0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5d96d",
   "metadata": {},
   "source": [
    "### Heatmap of the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix as heatmap\n",
    "plt.figure(figsize = (10,10))\n",
    "labels = ['sadness', 'anger', 'love', 'surprise', 'fear', 'joy']\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels, \n",
    "            annot=True, \n",
    "            cmap='Blues', \n",
    "            fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95d18cc4",
   "metadata": {
    "papermill": {
     "duration": 0.019959,
     "end_time": "2023-01-28T23:43:46.927110",
     "exception": false,
     "start_time": "2023-01-28T23:43:46.907151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 7 - Predicting new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f1636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T23:43:46.970316Z",
     "iopub.status.busy": "2023-01-28T23:43:46.969156Z",
     "iopub.status.idle": "2023-01-28T23:43:46.978772Z",
     "shell.execute_reply": "2023-01-28T23:43:46.977589Z"
    },
    "papermill": {
     "duration": 0.033872,
     "end_time": "2023-01-28T23:43:46.981454",
     "exception": false,
     "start_time": "2023-01-28T23:43:46.947582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_outcome(text):\n",
    "    # STEP1: make pre-processing\n",
    "    # replace everything in the text that is not a letter by space, in every row in column text \n",
    "    text = re.sub('[^a-zA-z]', ' ', text)\n",
    "    # convert text to tokens\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # remove stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stop_words.remove('not')  ## remove (not) from stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Word Normalization\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # convert tokens to text\n",
    "    text = ' '.join(tokens) \n",
    "    \n",
    "    # STEP2:\n",
    "    # converted text into a vector, part 3\n",
    "    text= [text]\n",
    "    text_x = best_pipeline.transform(text).toarray()\n",
    "    \n",
    "    # STEP3:\n",
    "    # predict method from nb (MultinomialNB), part 2\n",
    "    # predict new comment based on the trained model.\n",
    "    text_y = best_pipeline.predict(text_x)\n",
    "    \n",
    "    if (text_y == 0):\n",
    "        print(\"Sadness\")\n",
    "    elif (text_y ==1):\n",
    "        print(\"Anger\")\n",
    "    elif (text_y ==2):\n",
    "        print(\"Love\")\n",
    "    elif (text_y ==3):\n",
    "        print(\"Surprise\")\n",
    "    elif (text_y ==4):\n",
    "        print(\"Fear\")\n",
    "    elif (text_y ==5):\n",
    "        print(\"Joy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1aebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Prediction\n",
    "text = \"I am so happy\"\n",
    "pred_outcome(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of predictions\n",
    "best_pipeline.predict_proba(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbe730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Know the classes\n",
    "best_pipeline.classes_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ca5f85d",
   "metadata": {},
   "source": [
    "# Part 8 - Save the pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734323cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import joblib\n",
    "# reuse the same preprocessing steps and classifier on new data without having to redefine them each time.\n",
    "joblib.dump(best_pipeline,'pipeline.pkl') \n",
    "\n",
    "# load the model use joblib.load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 415.336838,
   "end_time": "2023-01-28T23:43:48.007017",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-28T23:36:52.670179",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
